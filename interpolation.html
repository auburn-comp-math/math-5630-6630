
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Interpolation &#8212; Introduction to Numerical Analysis I</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=7f8ff830"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"cO": "\\mathcal{O}", "fl": ["\\textrm{fl}(#1)", 1], "N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'interpolation';</script>
    <link rel="icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Root Finding" href="root_finding.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">Introduction to Numerical Analysis I</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="floating_point.html">Floating Point Arithmetic</a></li>
<li class="toctree-l1"><a class="reference internal" href="root_finding.html">Root Finding</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Interpolation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/interpolation.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Interpolation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#polynomial-interpolation">Polynomial Interpolation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lagrange-polynomial">Lagrange Polynomial</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpolation-error">Interpolation Error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#runge-s-phenomenon">Runge’s Phenomenon</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpolation-remainder-theory">Interpolation Remainder Theory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chebyshev-interpolation">Chebyshev Interpolation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stability-of-polynomial-interpolation">Stability of Polynomial Interpolation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-form">Newton Form</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hermite-polynomial-interpolation">Hermite Polynomial Interpolation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#trigonometric-interpolation">Trigonometric Interpolation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fourier-series">Fourier Series</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fast-fourier-transform">Fast Fourier Transform</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpolation-error-of-trigonometric-polynomial">Interpolation Error of Trigonometric Polynomial</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spline-interpolation">Spline Interpolation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-spline">Linear Spline</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="interpolation">
<h1>Interpolation<a class="headerlink" href="#interpolation" title="Link to this heading">#</a></h1>
<p>The interpolation (1D) solves problems of the following type:</p>
<blockquote>
<div><p>Given a set of predefined functions <span class="math notranslate nohighlight">\(\mathcal{K}\)</span>, find an element <span class="math notranslate nohighlight">\(f: \mathbb{I}\mapsto \mathbb{R}\)</span> in <span class="math notranslate nohighlight">\(\mathcal{K}\)</span> such that <span class="math notranslate nohighlight">\(y_j = f(x_j)\)</span> for all <span class="math notranslate nohighlight">\(j=0,\dots, n\)</span>.</p>
</div></blockquote>
<p>Here <span class="math notranslate nohighlight">\(\mathbb{I}\)</span> denotes a finite or infinite interval such that <span class="math notranslate nohighlight">\(x_1,\dots x_n\in \mathbb{I}\)</span>. One of the important applications for interpolation is Computer-Aided Design (CAD) which is used extensively in the manufacturing industry. Generally speaking, the interpolation provides a closed form of the function to determine the value of <span class="math notranslate nohighlight">\(y\)</span> where the parameter <span class="math notranslate nohighlight">\(x\)</span> is not accessible.</p>
<section id="polynomial-interpolation">
<h2>Polynomial Interpolation<a class="headerlink" href="#polynomial-interpolation" title="Link to this heading">#</a></h2>
<p>The polynomial interpolation considers the set <span class="math notranslate nohighlight">\(\mathcal{K} = \Pi_m\)</span>, where the set <span class="math notranslate nohighlight">\(\Pi_m\)</span> represents the polynomials of with degree <span class="math notranslate nohighlight">\( \le m\)</span>. We will seek for a polynomial <span class="math notranslate nohighlight">\(f(x)\)</span> with the constraints that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{cases}
        f\in \mathcal{K} = \Pi_n,&amp;\\
        f(x_k) = y_k &amp;\text{ for } k = 0, 1,\dots, n.
    \end{cases}
\end{split}\]</div>
<p>The points <span class="math notranslate nohighlight">\(x_k\)</span> are called <strong>interpolation nodes</strong>, if <span class="math notranslate nohighlight">\(m &gt; n\)</span> (resp. <span class="math notranslate nohighlight">\(m &lt; n\)</span>), the problem is underdetermined (resp. overdetermined). For the case that <span class="math notranslate nohighlight">\( m = n\)</span>, we have</p>
<div class="proof theorem admonition" id="THM-INTER-UNIQ">
<p class="admonition-title"><span class="caption-number">Theorem 4 </span></p>
<section class="theorem-content" id="proof-content">
<p>There exists a unique polynomial function <span class="math notranslate nohighlight">\(f\in \Pi_n\)</span> such that <span class="math notranslate nohighlight">\(f(x_j) = y_j\)</span> for <span class="math notranslate nohighlight">\(j=0,\dots, n\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. <strong>Existence</strong>: In order to construct the polynomial <span class="math notranslate nohighlight">\(f\)</span>, it is straightforward to consider the general form of polynomial <span class="math notranslate nohighlight">\(f(x) = \sum_{j=0}^n a_j x^j\)</span>, then we can formulate a linear system for the coefficients <span class="math notranslate nohighlight">\(a_j\)</span>, <span class="math notranslate nohighlight">\(j=0,\dots, n\)</span>, which is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
    1 &amp; x_0 &amp; x_0^2 &amp; \dots &amp; x_0^n \\
    1 &amp; x_1 &amp; x_1^2 &amp; \dots &amp; x_1^n \\
    \vdots &amp; \vdots &amp;\vdots &amp; \ddots &amp; \vdots\\
    1 &amp; x_n &amp; x_n^2 &amp; \dots &amp; x_n^n
\end{pmatrix} \begin{pmatrix}
    a_0\\a_1\\\vdots\\a_n
\end{pmatrix} = \begin{pmatrix}
    y_0\\y_1\\\vdots \\y_n
\end{pmatrix}.
\end{split}\]</div>
<p>The matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
V = \begin{pmatrix}
    1 &amp; x_0 &amp; x_0^2 &amp; \dots &amp; x_0^n \\
    1 &amp; x_1 &amp; x_1^2 &amp; \dots &amp; x_1^n \\
    \vdots &amp; \vdots &amp;\vdots &amp; \ddots &amp; \vdots\\
    1 &amp; x_n &amp; x_n^2 &amp; \dots &amp; x_n^n
\end{pmatrix} 
\end{split}\]</div>
<p>is called <strong>Vandermonde matrix</strong>. To determine the coefficients <span class="math notranslate nohighlight">\(a_j\)</span>, one needs the matrix <span class="math notranslate nohighlight">\(V\)</span> be invertible. Its determinant can be computed (as an exercise) as</p>
<div class="math notranslate nohighlight">
\[
        \det(V) = \prod_{0\le i\le j\le n}(x_j - x_i).
\]</div>
<p>When <span class="math notranslate nohighlight">\(x_j\)</span> are distinct, the determinant is nonzero.</p>
<p><strong>Uniqueness</strong>:  Suppose there are two distinct polynomials <span class="math notranslate nohighlight">\(f, g\in \Pi_n\)</span> satisfying the condition that <span class="math notranslate nohighlight">\(f(x_j) = g(x_j) = y_j\)</span>, then <span class="math notranslate nohighlight">\(f - g\)</span> has <span class="math notranslate nohighlight">\((n+1)\)</span> roots <span class="math notranslate nohighlight">\(x_j\)</span>, <span class="math notranslate nohighlight">\(j=0, \dots, n\)</span>. If <span class="math notranslate nohighlight">\(f\neq g\)</span>, it is clear that <span class="math notranslate nohighlight">\(f-g\in\Pi_n\)</span> has at most <span class="math notranslate nohighlight">\(n\)</span> roots. Contradiction.</p>
</div>
<p>In the above proof, the interpolation polynomial can be uniquely determined by solving the linear system</p>
<div class="math notranslate nohighlight">
\[\begin{split}        \begin{pmatrix}
    1 &amp; x_0 &amp; x_0^2 &amp; \dots &amp; x_0^n \\
    1 &amp; x_1 &amp; x_1^2 &amp; \dots &amp; x_1^n \\
    \vdots &amp; \vdots &amp;\vdots &amp; \ddots &amp; \vdots\\
    1 &amp; x_n &amp; x_n^2 &amp; \dots &amp; x_n^n
\end{pmatrix} \begin{pmatrix}
    a_0\\a_1\\\vdots\\a_n
\end{pmatrix} = \begin{pmatrix}
    y_0\\y_1\\\vdots \\y_n
\end{pmatrix}.
\end{split}\]</div>
<p>However, it is generally easier to compute the polynomial <span class="math notranslate nohighlight">\(f\)</span> with the <strong>Lagrange polynomial interpolation</strong> (which is somewhat equivalent to compute the inverse of <span class="math notranslate nohighlight">\(V\)</span>).</p>
<section id="lagrange-polynomial">
<h3>Lagrange Polynomial<a class="headerlink" href="#lagrange-polynomial" title="Link to this heading">#</a></h3>
<div class="proof definition admonition" id="DEF-LA-PO">
<p class="admonition-title"><span class="caption-number">Definition 2 </span></p>
<section class="definition-content" id="proof-content">
<p>For the given distinct <span class="math notranslate nohighlight">\(x_j\)</span>, <span class="math notranslate nohighlight">\(j = 0, 1, \dots, n\)</span>, the <span class="math notranslate nohighlight">\((n+1)\)</span> Lagrange polynomials <span class="math notranslate nohighlight">\(L_0, L_1,\dots, L_n\in\Pi_n\)</span> are defined by</p>
<div class="math notranslate nohighlight">
\[
    L_j(x) = \prod_{s = 0, s\neq j}^n \frac{x - x_s}{x_j - x_s}, \quad j = 0, 1,\dots , n.
\]</div>
</section>
</div><p>It is clear that these polynomials satisfy the conditions that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    L_j(x_k) = \delta_{jk} := \begin{cases}
        1&amp;\text{for } k=j,\\
        0&amp;\text{for } k\neq j.
    \end{cases}
\end{split}\]</div>
<p>Therefore these polynomials are linearly independent, which form a basis of the <span class="math notranslate nohighlight">\((n+ 1)\)</span>-dimensional space <span class="math notranslate nohighlight">\(\Pi_n\)</span>.</p>
<div class="proof theorem admonition" id="THM-UNIQ-LAG-INTER">
<p class="admonition-title"><span class="caption-number">Theorem 5 </span></p>
<section class="theorem-content" id="proof-content">
<p>The unique interpolating polynomial <span class="math notranslate nohighlight">\(f\)</span> satisfying <span class="math notranslate nohighlight">\(f(x_j) = y_j\)</span>, <span class="math notranslate nohighlight">\(j=0,1,\dots, n\)</span> can be represented by</p>
<div class="math notranslate nohighlight">
\[
    f(x) = \sum_{j=0}^n y_j L_j(x).
\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. It is straightforward to check the interpolation conditions are satisfied.</p>
</div>
<div class="proof remark admonition" id="remark-3">
<p class="admonition-title"><span class="caption-number">Remark 4 </span></p>
<section class="remark-content" id="proof-content">
<p>We introduce a preliminary procedure to compute value of the interpolating polynomial <span class="math notranslate nohighlight">\(f\)</span> at a point <span class="math notranslate nohighlight">\(x\)</span>. Let constants <span class="math notranslate nohighlight">\(k_j\)</span> and <span class="math notranslate nohighlight">\(q(x)\)</span> be defined as</p>
<div class="math notranslate nohighlight">
\[
    k_j = \prod_{s= 0, s\neq j}\frac{1}{x_j - x_s},\quad q(x) = \prod_{j=0}^n (x - x_j),
\]</div>
<p>then</p>
<div class="math notranslate nohighlight">
\[
    f(x) = \sum_{j=0}^n y_j L_j(x) = q(x)  \sum_{j=0}^n k_j y_j \frac{1}{x - x_j}.
\]</div>
<p>One can first compute <span class="math notranslate nohighlight">\(k_j\)</span> with <span class="math notranslate nohighlight">\(\cO(n^2)\)</span> flops, then <span class="math notranslate nohighlight">\(f(x)\)</span> can be computed with <span class="math notranslate nohighlight">\(\cO(n)\)</span> flops. The advantage of the above scheme is the constants <span class="math notranslate nohighlight">\(k_j\)</span> are independent of <span class="math notranslate nohighlight">\(y_j\)</span>, therefore evaluating another instance of the interpolating polynomial will not need to re-compute them. The disadvantage is that if we add a new node, the constants <span class="math notranslate nohighlight">\(k_j\)</span> have to be updated with an additional cost of <span class="math notranslate nohighlight">\(\cO(n)\)</span> flops. Later we will see the Newton’s form can overcome this issue.</p>
</section>
</div></section>
<section id="interpolation-error">
<h3>Interpolation Error<a class="headerlink" href="#interpolation-error" title="Link to this heading">#</a></h3>
<p>When the data pairs <span class="math notranslate nohighlight">\((x_j, y_j)\)</span>, <span class="math notranslate nohighlight">\(j=0,1,\dots, n\)</span> are generated by a sufficiently smooth function <span class="math notranslate nohighlight">\(h(x)\)</span>, it is possible to quantify the error between the interpolating polynomial <span class="math notranslate nohighlight">\(f(x)\)</span> and <span class="math notranslate nohighlight">\(h(x)\)</span>.</p>
<div class="proof theorem admonition" id="THM-INTERP-ERROR">
<p class="admonition-title"><span class="caption-number">Theorem 6 </span></p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(h: [a, b]\mapsto \mathbb{R}\)</span> be a <span class="math notranslate nohighlight">\((n+1)\)</span>-times differentiable function. If <span class="math notranslate nohighlight">\(f(x)\in\Pi_n\)</span> is the interpolating polynomial that</p>
<div class="math notranslate nohighlight">
\[
f(x_j) = h(x_j),
\]</div>
<p>for <span class="math notranslate nohighlight">\(j=0,1,\dots, n\)</span>. Then for each <span class="math notranslate nohighlight">\(\overline{x}\in [a, b]\)</span>, the error can be represented by</p>
<div class="math notranslate nohighlight">
\[
h(\overline{x}) - f(\overline{x}) = \frac{\omega(\overline{x})}{(n+1)!} h^{(n+1)}(\xi),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\xi = \xi(\overline{x})\in [a, b]\)</span> and <span class="math notranslate nohighlight">\(\omega(x) = \prod_{j=0}^n (x - x_j)\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The proof is based on the Rolle’s Theorem. Select any <span class="math notranslate nohighlight">\(\overline{x}\in[a, b]\)</span> such that <span class="math notranslate nohighlight">\(\omega(\overline{x})\neq 0\)</span>, then let</p>
<div class="math notranslate nohighlight">
\[
\psi(x) = h(x) - f(x) - k\omega (x)
\]</div>
<p>the constant <span class="math notranslate nohighlight">\(k\)</span> is chosen such that <span class="math notranslate nohighlight">\(\psi(\overline{x}) = 0\)</span>. Then <span class="math notranslate nohighlight">\(\psi(x) = 0\)</span> at <span class="math notranslate nohighlight">\((n+2)\)</span> points</p>
<div class="math notranslate nohighlight">
\[
x_0, x_1, \dots, x_n, \overline{x}\in [a, b]
\]</div>
<p>By Rolle’s Theorem, <span class="math notranslate nohighlight">\(\psi^{(n+1)}\)</span> has at least one zero <span class="math notranslate nohighlight">\(\xi\)</span> in <span class="math notranslate nohighlight">\([a,b]\)</span>. Therefore</p>
<div class="math notranslate nohighlight">
\[
    \psi^{(n+1)}(\xi) = h^{(n+1)}(\xi) - 0 - k(n+1)! = 0.
\]</div>
</div>
<div class="proof corollary admonition" id="corollary-5">
<p class="admonition-title"><span class="caption-number">Corollary 2 </span></p>
<section class="corollary-content" id="proof-content">
<p>If <span class="math notranslate nohighlight">\(h(x)\in C^{\infty}([a, b])\)</span> satisfies that <span class="math notranslate nohighlight">\(\max_{x\in[a,b]} |h^{(n)}(x)|\le M &lt;\infty\)</span> for all <span class="math notranslate nohighlight">\(n\ge 0\)</span>, then the interpolating polynomial approximates <span class="math notranslate nohighlight">\(h\)</span> uniformly as the number of nodes <span class="math notranslate nohighlight">\(n\to \infty\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Since <span class="math notranslate nohighlight">\(|x -x_j|\le b-a\)</span>, the error is bounded by <span class="math notranslate nohighlight">\(\frac{(b-a)^{n+1}}{(n+1)!} M\)</span>, which converges to zero.</p>
</div>
<p>It is interesting to think about the converse: under what kind of condition the interpolation error is not vanishing as the number of nodes tends to infinity? From the <a class="reference internal" href="#THM-INTERP-ERROR">Theorem 6</a>, the error depends on the sizes of three terms.</p>
<ul class="simple">
<li><p>The bound of the <span class="math notranslate nohighlight">\((n+1)\)</span>-th derivative, <span class="math notranslate nohighlight">\(\max_{x\in[a,b]}|h^{(n+1)}(x)|\)</span>. This could grow rapidly. For instance, <span class="math notranslate nohighlight">\(h(x) = 1/\sqrt{x}\)</span> on <span class="math notranslate nohighlight">\([\frac{1}{2}, \frac{3}{2}]\)</span>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
h^{(n+1)}(x) = \frac{(-1)^{n+1}}{2^{n+1}} (2n+1)!! x^{-(2n+3)/2}.
\]</div>
<ul class="simple">
<li><p>The function <span class="math notranslate nohighlight">\(\omega(x) = \prod_{j=0}^n (x - x_j)\)</span>, such product could be large if <span class="math notranslate nohighlight">\(x\)</span> and the nodes <span class="math notranslate nohighlight">\(x_j\)</span> are not so close.</p></li>
<li><p>The term <span class="math notranslate nohighlight">\(\frac{1}{(n+1)!}\)</span>, which decays fast.</p></li>
</ul>
<p>We can see that for the function <span class="math notranslate nohighlight">\(h(x) = 1/\sqrt{x}\)</span> on <span class="math notranslate nohighlight">\([\frac{1}{2}, \frac{3}{2}]\)</span>, it is not trivial to show the interpolating polynomial could converge to <span class="math notranslate nohighlight">\(h\)</span> anymore (it is still true for certain choices of <span class="math notranslate nohighlight">\(x_j\)</span>).</p>
<p>Next, we try to provide a better estimate of <span class="math notranslate nohighlight">\(\omega\)</span> for the special choice: equally spaced nodes.  Let the nodes <span class="math notranslate nohighlight">\(x_j = a + j\Delta\)</span>, where <span class="math notranslate nohighlight">\(\Delta = \frac{b-a}{n}\)</span>. It is not difficult (prove it) to see <span class="math notranslate nohighlight">\(\omega(x)\)</span> will be the worst if <span class="math notranslate nohighlight">\(x\)</span> is located on the end sub-intervals, <span class="math notranslate nohighlight">\([x_0, x_1]\)</span> and <span class="math notranslate nohighlight">\([x_{n-1}, x_n]\)</span>. Without loss of generality, we assume <span class="math notranslate nohighlight">\(x\)</span> is located on <span class="math notranslate nohighlight">\([x_0, x_1]\)</span>, then</p>
<div class="math notranslate nohighlight">
\[|x - x_j|\le j \Delta\]</div>
<p>for <span class="math notranslate nohighlight">\(j = 2, \dots, n\)</span>, which implies</p>
<div class="math notranslate nohighlight">
\[
    |\omega(x)|\le \prod_{j=0}^n |x - x_j|\le n! \Delta^{n-1} \sup_{x\in [x_0, x_1]} |(x - x_0)(x-x_1)|= \frac{n!}{4} \frac{(b-a)^{n+1}}{n^{n+1}}.
\]</div>
<p>Thus the interpolation error is bounded by</p>
<div class="math notranslate nohighlight">
\[  \|h  - f\|_{\infty} \le  \frac{\sup_{x\in[a,b]}|h^{(n+1)}(x)|}{4(n+1)} \frac{(b-a)^{n+1}}{n^{n+1}}.\]</div>
<p>Such an estimate is useful to derive uniform convergence.</p>
<div class="proof example admonition" id="example-6">
<p class="admonition-title"><span class="caption-number">Example 2 </span></p>
<section class="example-content" id="proof-content">
<p>Consider <span class="math notranslate nohighlight">\(h(x) = 1/x\)</span> on <span class="math notranslate nohighlight">\([\frac{1}{2}, \frac{3}{2}]\)</span>. Then <span class="math notranslate nohighlight">\(h^{(n+1)}(x) = \frac{(n+1)!(-1)^{n+1}}{x^{n+2}}\)</span>, hence</p>
<div class="math notranslate nohighlight">
\[
    \frac{| h^{(n+1)}(x) |}{4(n+1)}  \frac{(b-a)^{n+1}}{n^{n+1}} \le \frac{1}{4n^{n+1}}\max_{x\in[\frac{1}{2}, \frac{3}{2}]} \left| \frac{1}{x^{n+2}} \right| = \frac{1}{2} \left(\frac{2}{n}\right)^{n+1},
\]</div>
<p>therefore, the interpolation error converges to zero exponentially. It is important to notice that the above method only works for intervals away from the origin.</p>
</section>
</div></section>
<section id="runge-s-phenomenon">
<h3>Runge’s Phenomenon<a class="headerlink" href="#runge-s-phenomenon" title="Link to this heading">#</a></h3>
<p>From the above discussion, we can see there is a possibility that <span class="math notranslate nohighlight">\(\max_{x\in[a,b]}|h^{n+1}(x)|\omega(x)\)</span> grows faster than <span class="math notranslate nohighlight">\((n+1)!\)</span>, which would lead to divergence. Hence increasing the number of interpolation nodes (at least for equally spaced nodes) is not guaranteed for better approximation. The most famous example is the one made by Carl Runge.</p>
<div class="math notranslate nohighlight" id="equation-eq-runge-example">
<span class="eqno">(1)<a class="headerlink" href="#equation-eq-runge-example" title="Link to this equation">#</a></span>\[h(x) = \frac{1}{1+x^2},\quad x\in [-5, 5].\]</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<figure class="align-default" id="id1">
<img alt="_images/a6ae6c71b63ce596543dfc4fd27e1957e16bfc4da50a585a774f253036bace86.png" class="align-center" src="_images/a6ae6c71b63ce596543dfc4fd27e1957e16bfc4da50a585a774f253036bace86.png" />
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Runge phenomenon with 11 equally spaced nodes</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<p>It can be shown that the interpolation will diverge at around <span class="math notranslate nohighlight">\(3.6\)</span> as <span class="math notranslate nohighlight">\(n\to \infty\)</span> and the maximum error <span class="math notranslate nohighlight">\(\max_{x\in[-5, 5]} |f_n(x) - h(x) |\)</span> grows exponentially, where <span class="math notranslate nohighlight">\(f_n\)</span> is the interpolating polynomial with <span class="math notranslate nohighlight">\(n+1\)</span> equally spaced nodes.</p>
</section>
<section id="interpolation-remainder-theory">
<h3>Interpolation Remainder Theory<a class="headerlink" href="#interpolation-remainder-theory" title="Link to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(f_n\)</span> be the degree-<span class="math notranslate nohighlight">\(n\)</span> polynomial interpolates <span class="math notranslate nohighlight">\(h\)</span> at nodes <span class="math notranslate nohighlight">\(\{x_j\}_{j=0}^n\)</span>. If <span class="math notranslate nohighlight">\(h\)</span> is analytic in a domain <span class="math notranslate nohighlight">\(T\)</span> (possibly contains holes),  then the interpolation (Lagrange interpolant) can be written as</p>
<div class="math notranslate nohighlight">
\[
    f_n(z) = \sum_{j=0}^n \frac{\omega(z) h(x_j)}{(z - x_j) \omega'(x_j)}
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\psi(\xi; z) = \frac{(\omega(\xi) - \omega(z)) h(\xi)}{(\xi - z) \omega(\xi)}\)</span>, then by the Residue theorem for simple poles, if <span class="math notranslate nohighlight">\(z \neq x_j\)</span>,</p>
<div class="math notranslate nohighlight">
\[
    \frac{1}{2\pi i}\int_{\partial T} \psi(\xi; z) d\xi = \sum_{j=0}^n \mathrm{Res}(\psi, x_j) = \sum_{j=0}^n \frac{(\omega(x_j) - \omega(z)) h(x_j)}{(x_j - z)\omega'(x_j)} = f_n(z),
\]</div>
<p>which implies that</p>
<div class="math notranslate nohighlight">
\[
    h(z) - f_n(z) = \frac{1}{2\pi i}\int_{\partial T} \frac{\omega(z) h(\xi)}{(\xi - z)\omega(\xi)} d\xi.
\]</div>
<p>The error analysis focuses on studying the behavior of <span class="math notranslate nohighlight">\(|\omega(z)|\)</span> as <span class="math notranslate nohighlight">\(n\to \infty\)</span>.</p>
<div class="proof lemma admonition" id="Lem-2-Ome-Lim">
<p class="admonition-title"><span class="caption-number">Lemma 2 </span></p>
<section class="lemma-content" id="proof-content">
<p>If <span class="math notranslate nohighlight">\(\{x_j\}_{j=0}^n\)</span> are equispaced nodes over <span class="math notranslate nohighlight">\([a, b]\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
\lim_{n\to\infty} |\omega(z)|^{\frac{1}{n+1}} = \exp\left(\frac{1}{b - a}\int_a^b \log|z-\xi| d\xi \right).
\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Taking <span class="math notranslate nohighlight">\(\log\)</span> on <span class="math notranslate nohighlight">\(|\omega|^{\frac{1}{n+1}}\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
\log |\omega|^{\frac{1}{n+1}} = \frac{1}{n+1}\sum_{j=0}^n \log |z - x_j|\to \frac{1}{b-a}\int_a^b \log|z - \xi| d\xi. 
\]</div>
</div>
<p>Let <span class="math notranslate nohighlight">\(\sigma_n(z):= |\omega(z)|^{\frac{1}{n+1}}\)</span> and define the contour <span class="math notranslate nohighlight">\(C_{\rho} = \{z\in \mathbb{C}\mid \sigma_n(z) = \rho\}\)</span>. These level sets are concentric closed curves about the midpoint of <span class="math notranslate nohighlight">\([a,b]\)</span>.</p>
<div class="proof lemma admonition" id="Lem-2-Ana-Uni-Con">
<p class="admonition-title"><span class="caption-number">Lemma 3 </span></p>
<section class="lemma-content" id="proof-content">
<p>Suppose the interpolation nodes <span class="math notranslate nohighlight">\(\{x_j\}_{j=0}^n\)</span> are enclosed by <span class="math notranslate nohighlight">\(C_{\rho}\)</span> and <span class="math notranslate nohighlight">\(h\)</span> is analytic inside <span class="math notranslate nohighlight">\(C_{\rho}\)</span>. Let <span class="math notranslate nohighlight">\(z\in C_{\rho'}\)</span> be such that <span class="math notranslate nohighlight">\(\rho'&lt;\rho\)</span>, then <span class="math notranslate nohighlight">\(f_n\to h\)</span> uniformly as <span class="math notranslate nohighlight">\(n\to\infty\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Using the maximum modulus principle, the analytic function <span class="math notranslate nohighlight">\(h - f_n\)</span> must attain its maximum modulus at the boundary <span class="math notranslate nohighlight">\(C_{\rho}\)</span>, thus</p>
<div class="math notranslate nohighlight">
\[
|h(z) - f_n(z)| = \frac{1}{2\pi}\sup_{z\in C_{\rho'}} \left|\int_{C_{\rho}} \frac{\omega(z)}{\omega(\xi)} \frac{h(\xi)}{(\xi - z)} d\xi\right| \le C(\rho, \rho') \sup_{\xi\in C_{\rho}} \frac{|\omega(z)|}{|\omega(\xi)|},
\]</div>
<p>where <span class="math notranslate nohighlight">\(C(\rho, \rho')\)</span> is a positive constant independent of <span class="math notranslate nohighlight">\(n\)</span>. For <span class="math notranslate nohighlight">\(n\)</span> sufficiently large, we can find <span class="math notranslate nohighlight">\(0 &lt; \delta &lt; \frac{1}{3}(\rho - \rho')\)</span> sufficiently small such that</p>
<div class="math notranslate nohighlight">
\[
\sup_{\xi\in C_{\rho}} \frac{|\omega(z)|}{|\omega(\xi)|} \le \left|\frac{\rho' + \delta}{\rho - \delta}\right|^{n+1} \to 0\; \text{ as }n\to \infty. 
\]</div>
</div>
<p>When <span class="math notranslate nohighlight">\(h\)</span> is not analytic inside <span class="math notranslate nohighlight">\(C_{\rho}\)</span>, let us consider a generic situation in which there exist isolated simple poles <span class="math notranslate nohighlight">\(z_k\in C_{\rho_k}\)</span>, <span class="math notranslate nohighlight">\(k\in [m]\)</span> with <span class="math notranslate nohighlight">\(\rho_k &lt; \rho\)</span>, then we select a contour <span class="math notranslate nohighlight">\(C_{\rho'}\)</span> that <span class="math notranslate nohighlight">\(\rho_k &lt;\rho'&lt;\rho\)</span> for all <span class="math notranslate nohighlight">\(k\)</span>. For <span class="math notranslate nohighlight">\(z\in C_{\rho'}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    h(z) - f_n(z) &amp;= \frac{1}{2\pi i} \int_{C_{\rho} - \bigcup_{k=1}^m \Gamma_k} \frac{\omega(z)h(\xi)}{(\xi - z) \omega(\xi)} d\xi \\
    &amp;= \frac{1}{2\pi i} \int_{C_{\rho}} \frac{\omega(z)h(\xi)}{(\xi - z) \omega(\xi)} d\xi - \frac{1}{2\pi i}\sum_{k=1}^m \int_{\Gamma_k}  \frac{\omega(z)h(\xi)}{(\xi - z) \omega(\xi)} d\xi,
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Gamma_k\)</span> is a small path surrounding <span class="math notranslate nohighlight">\(z_k\)</span>. The first term can be estimated using the lemma~\ref{Lem: 2-Ana-Uni-Con} whose limit goes to zero as <span class="math notranslate nohighlight">\(n\to \infty\)</span>. The second term is the summation</p>
<div class="math notranslate nohighlight">
\[
\sum_{k=1}^m \frac{\omega(z)}{\omega(z_k)}\frac{\mathrm{Res}(h, z_k)}{z_k - z}.
\]</div>
<p>Since for sufficiently large <span class="math notranslate nohighlight">\(n\)</span>, we can find <span class="math notranslate nohighlight">\(0&lt;\delta&lt;\min_{k\in[m]}\frac{1}{3}(\rho'-\rho_k)\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\left|\frac{\omega(z)}{\omega(z_k)}\right|^{\frac{1}{n+1}} = \frac{\sigma_n(z)}{\sigma_n(z_k)} &gt; \min_{k\in [m]}\frac{\rho'-\delta}{\rho_k + \delta} &gt; 1.
\]</div>
<p>Define the unique set <span class="math notranslate nohighlight">\(\mathcal{U} = \{u_1, u_2,\cdots, u_l\}\)</span> that <span class="math notranslate nohighlight">\(u_1 &lt; u_2&lt;\cdots &lt; u_l\)</span> which consists of all distinct values from <span class="math notranslate nohighlight">\(\{ \rho_1, \rho_2, \cdots,  \rho_m\}\)</span>, then the summation can be decomposed into <span class="math notranslate nohighlight">\(l\)</span> groups:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\sum_{k=1}^m \frac{\omega(z)}{\omega(z_k)}\frac{\mathrm{Res}(h, z_k)}{z_k - z} = \sum_{s=1}^l \sum_{\substack{k\in [m] \\ \rho_k = u_s}} \frac{\omega(z)}{\omega(z_k)}\frac{\mathrm{Res}(h, z_k)}{z_k - z}.
\end{split}\]</div>
<p>As <span class="math notranslate nohighlight">\(n\to\infty\)</span>, if any of the groups does not vanish, then the whole summation must blow up as <span class="math notranslate nohighlight">\(n\to\infty\)</span> (why?). Otherwise, the limiting summation should vanish, which violates the maximum modulus principle.</p>
<p>In Runge’s example <a class="reference internal" href="#equation-eq-runge-example">(1)</a>, the simple poles <span class="math notranslate nohighlight">\(\pm i\)</span> are on the contour <span class="math notranslate nohighlight">\(C_{\rho}\)</span> which intersects the real line at <span class="math notranslate nohighlight">\(x_c\approx 3.6334\)</span>. Therefore, for <span class="math notranslate nohighlight">\(|x| &lt; x_c\)</span>, the interpolation <span class="math notranslate nohighlight">\(f_n\)</span> uniformly converges to <span class="math notranslate nohighlight">\(h\)</span> and diverges once <span class="math notranslate nohighlight">\(|x| &gt; x_c\)</span>.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<figure class="align-default" id="id2">
<img alt="_images/322d862b17b1e0738f72ccbdeedb1e20ac4d5153be8cc0c98d2fdb57530ec243.png" class="align-center" src="_images/322d862b17b1e0738f72ccbdeedb1e20ac4d5153be8cc0c98d2fdb57530ec243.png" />
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">Contour curve passing through the poles at <span class="math notranslate nohighlight">\(\pm i\)</span>.</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<p>There exist better choices of interpolation nodes to prevent such a phenomenon. We will discuss this topic in the next Section.</p>
</section>
<section id="chebyshev-interpolation">
<h3>Chebyshev Interpolation<a class="headerlink" href="#chebyshev-interpolation" title="Link to this heading">#</a></h3>
<p>The Chebyshev interpolation aims to minimize the bound of the interpolation error. The bound of <span class="math notranslate nohighlight">\(\omega(x)\)</span> only depends on the choice of the nodes, so a natural question is: what kind of interpolation nodes will minimize
<span class="math notranslate nohighlight">\(\max_{x\in [a, b]} \prod_{j=0}^n |x-x_j|\)</span>. We first restrict our analysis to the interval <span class="math notranslate nohighlight">\([a, b] = [-1,1]\)</span> for simplicity, the general case will be discussed later.</p>
<div class="proof example admonition" id="example-9">
<p class="admonition-title"><span class="caption-number">Example 3 </span></p>
<section class="example-content" id="proof-content">
<p>When <span class="math notranslate nohighlight">\(n = 1\)</span>, <span class="math notranslate nohighlight">\(\omega(x) = (x - x_0)(x - x_1)\)</span>, this function changes sign over the sub-intervals <span class="math notranslate nohighlight">\([-1, x_0)\)</span>, <span class="math notranslate nohighlight">\((x_0, x_1)\)</span>, <span class="math notranslate nohighlight">\((x_1, 1]\)</span>, then we can compute the maximum of <span class="math notranslate nohighlight">\(|\omega(x)|\)</span> on these sub-intervals. Therefore we need to solve</p>
<div class="math notranslate nohighlight">
\[
    \min_{x_0, x_1\in [-1,1]}\max((1 + x_0)(1 + x_1), \frac{(x_1-x_0)^2}{4}, (1 - x_0)(1 - x_1) ),
\]</div>
<p>while we can observe that</p>
<div class="math notranslate nohighlight">
\[
    \frac{1}{2} (1 + x_0)(1 + x_1) +  \frac{(x_1-x_0)^2}{4} + \frac{1}{2}(1 - x_0)(1 - x_1) = 1 + \frac{(x_0 + x_1)^2}{4}\ge 1
\]</div>
<p>holds for any choice of <span class="math notranslate nohighlight">\(x_0, x_1\)</span>, which means the maximum is at least <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span>, it occurs when all terms are equal and <span class="math notranslate nohighlight">\(x_0 + x_1 = 0\)</span>. Hence <span class="math notranslate nohighlight">\(x_0 = -\frac{\sqrt{2}}{2}, x_1 = \frac{\sqrt{2}}{2}\)</span>.</p>
</section>
</div><div class="proof definition admonition" id="definition-10">
<p class="admonition-title"><span class="caption-number">Definition 3 </span></p>
<section class="definition-content" id="proof-content">
<p>The Chebyshev polynomials of the first kind are defined by:</p>
<div class="math notranslate nohighlight">
\[
T_k(x) = \cos (k\arccos x),\quad x\in[-1,1].
\]</div>
</section>
</div><div class="proof theorem admonition" id="theorem-11">
<p class="admonition-title"><span class="caption-number">Theorem 7 </span></p>
<section class="theorem-content" id="proof-content">
<p>The Chebyshev polynomial satisfies the following:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(T_k(\cos\theta) = \cos k\theta, \quad \theta\in [0, \pi]\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(T_0 \equiv 1\)</span>, <span class="math notranslate nohighlight">\(T_1(x) = x\)</span> and <span class="math notranslate nohighlight">\(T_{k+1}(x) = 2 x T_{k}(x) - T_{k-1}(x), \quad k\ge 1.\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\max_{x\in[-1,1]} |T_k(x)| = 1\)</span>.</p></li>
<li><p>The leading coefficient of <span class="math notranslate nohighlight">\(T_k(x)\)</span> is <span class="math notranslate nohighlight">\(2^{k-1}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(T_k\)</span> has a total of <span class="math notranslate nohighlight">\((k+1)\)</span> extrema <span class="math notranslate nohighlight">\(s_j = \cos(\frac{j\pi}{k}), j = 0, 1,\dots, n\)</span> in the interval <span class="math notranslate nohighlight">\([-1,1]\)</span> such that <span class="math notranslate nohighlight">\(T_k(s_j) = (-1)^j\)</span>.</p></li>
</ul>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The first three statements are straightforward after replacing the variable <span class="math notranslate nohighlight">\(x = \cos\theta\)</span>. The fourth statement is an immediate result with induction through the recursion formula
<span class="math notranslate nohighlight">\(T_{k+1}(x) = 2 x T_k(x) - T_{k-1}\)</span>. The last statement is trivial.</p>
</div>
<p>More importantly, the Chebyshev polynomial has the following optimality property.</p>
<div class="proof theorem admonition" id="theorem-12">
<p class="admonition-title"><span class="caption-number">Theorem 8 </span></p>
<section class="theorem-content" id="proof-content">
<p>The optimal choice of interpolation nodes that minimize <span class="math notranslate nohighlight">\(\max |\omega(x)|\)</span> are the extrema of Chebyshev polynomial <span class="math notranslate nohighlight">\(T_{{n+1}}\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-cheby">
<span class="eqno">(2)<a class="headerlink" href="#equation-eq-cheby" title="Link to this equation">#</a></span>\[\min_{x_j\in[-1,1]} \max_{x\in[-1,1]} |\omega(x)| =   \max_{x\in[-1,1]} \frac{1}{2^n}|T_{n+1}(x)|  = \frac{1}{2^n}\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Let the roots of <span class="math notranslate nohighlight">\(T_{n+1}(x)\)</span> be <span class="math notranslate nohighlight">\(z_0, z_1, \dots, z_{n}\in [-1, 1]\)</span>, then we can write</p>
<div class="math notranslate nohighlight">
\[T_{n+1} = 2^{n}(x - z_0)(x-z_1)\dots (x - z_{n})\]</div>
<p>therefore <span class="math notranslate nohighlight">\(\frac{1}{2^n} T_{n+1}(x)\)</span> is a polynomial with leading coefficient as <span class="math notranslate nohighlight">\(1\)</span>. Since <span class="math notranslate nohighlight">\(\max_{x\in[-1,1]} |T_{n+1}(x)| = 1\)</span>, it is clear that <span class="math notranslate nohighlight">\(\max_{x\in [-1,1]} \frac{1}{2^n}|T_{n+1}(x)| = \frac{1}{2^n}\)</span>, which is the second equality in <a class="reference internal" href="#equation-eq-cheby">(2)</a>. For the first equality, we try to prove by contradiction. Let <span class="math notranslate nohighlight">\(x_0, x_1, \dots, x_n\in [-1, 1]\)</span>, such that</p>
<div class="math notranslate nohighlight">
\[\max_{x\in[-1,1]}|\omega(x)| &lt; \frac{1}{2^n},\]</div>
<p>then we define the polynomial <span class="math notranslate nohighlight">\(\psi(x) = \frac{1}{2^n}T_{n+1}(x)- \omega(x)\)</span>, its degree is at most <span class="math notranslate nohighlight">\(n\)</span> due to cancellation, therefore at most have <span class="math notranslate nohighlight">\(n\)</span> zeros. On the other hand, because <span class="math notranslate nohighlight">\(\frac{1}{2^n}T_{n+1}(s_j) = \frac{1}{2^n}(-1)^j\)</span> at the extrema <span class="math notranslate nohighlight">\(s_j = \cos(\frac{j\pi}{n+1})\)</span>, <span class="math notranslate nohighlight">\(j=0,\dots, (n+1)\)</span>, the polynomial <span class="math notranslate nohighlight">\(\psi(s_j)\)</span> must share the same sign of <span class="math notranslate nohighlight">\(\frac{1}{2^n}T_{n+1}(s_j)\)</span>. This means <span class="math notranslate nohighlight">\(\psi(x)\)</span> changes sign <span class="math notranslate nohighlight">\((n+1)\)</span> times, hence <span class="math notranslate nohighlight">\((n+1)\)</span> zeros. It is a contradiction.</p>
</div>
<div class="proof definition admonition" id="definition-13">
<p class="admonition-title"><span class="caption-number">Definition 4 </span></p>
<section class="definition-content" id="proof-content">
<p>The interpolation nodes <span class="math notranslate nohighlight">\(z_j = \cos(\frac{(2j+1)\pi}{2(n+1)})\)</span>, <span class="math notranslate nohighlight">\(j = 0, 1, \dots, n\)</span> are called <strong>Chebyshev nodes</strong>. These nodes are the zeros of Chebyshev polynomial <span class="math notranslate nohighlight">\(T_{n+1}\)</span>.</p>
</section>
</div><p>Now we can generalize the above theorem to interval <span class="math notranslate nohighlight">\([a, b]\)</span>. One can defined the affine transformation <span class="math notranslate nohighlight">\(\phi\)</span> mapping <span class="math notranslate nohighlight">\([-1,1]\)</span> to <span class="math notranslate nohighlight">\([a, b]\)</span> by <span class="math notranslate nohighlight">\(\phi(x) = \frac{1}{2} (a + b + (b-a)x)\)</span>. It is not difficult to prove the following.</p>
<div class="proof corollary admonition" id="COR-CHEBY">
<p class="admonition-title"><span class="caption-number">Corollary 3 </span></p>
<section class="corollary-content" id="proof-content">
<p>The optimal choice of interpolation nodes that minimize <span class="math notranslate nohighlight">\(\max |\omega(x)|\)</span> on <span class="math notranslate nohighlight">\([a, b]\)</span> are <span class="math notranslate nohighlight">\(\phi(z_j)\)</span> and</p>
<div class="math notranslate nohighlight">
\[\renewcommand{\eps}{\varepsilon}
\min_{x_j\in [a, b]} \max_{x\in [a, b]} |\omega(x)| = \frac{(b-a)^{n+1}}{2\cdot 4^n}.
\]</div>
<p>This bound is much smaller than the bound for equally spaced nodes.</p>
</section>
</div></section>
<section id="stability-of-polynomial-interpolation">
<h3>Stability of Polynomial Interpolation<a class="headerlink" href="#stability-of-polynomial-interpolation" title="Link to this heading">#</a></h3>
<p>Suppose there is some perturbation of the data <span class="math notranslate nohighlight">\(\tilde{y}_j = y_j + \eps_j\)</span> at the interpolation node <span class="math notranslate nohighlight">\(x_j\)</span>. Let <span class="math notranslate nohighlight">\(\tilde{f}_n(x)\)</span> and <span class="math notranslate nohighlight">\(f_n(x)\)</span> be the interpolating polynomials on perturbed data and original data. Then with Lagrange polynomials,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
|f_n(x) - \tilde{f_n}(x)| &amp;= |\sum_{j=0}^n (y_j - \tilde{y}_j) L_j(x)| \\
&amp;\le \left(\max_{j} |\eps_j|\right) \sum_{j=0}^n |L_j(x)|.
\end{aligned}
\end{split}\]</div>
<p>Here <span class="math notranslate nohighlight">\(\lambda_n(x) := \sum_{j=0}^n |L_j(x)|\)</span> is the <strong>Lebesgue function</strong>. It is a piecewise defined polynomial. Its maximum <span class="math notranslate nohighlight">\(\Lambda_n\)</span> is the <strong>Lebesgue constant</strong> and it only depends on the choice of interpolation nodes. For the equally spaced nodes, this Lebesgue constant grows exponentially,</p>
<div class="math notranslate nohighlight">
\[
\Lambda_{n, equal} \sim \frac{2^{n+1}}{en \log n}.
\]</div>
<p>For the general case, it has been proved by Paul Erdös (1964) that there exists a constant <span class="math notranslate nohighlight">\(C &gt; 0\)</span></p>
<div class="math notranslate nohighlight">
\[
    \Lambda_n &gt; \frac{2}{\pi}\log(n+1) - C,\quad  n\ge 0,
\]</div>
<p>As the number of nodes <span class="math notranslate nohighlight">\(n\to \infty\)</span>, <span class="math notranslate nohighlight">\(\Lambda_n \to \infty\)</span>. This leads to the result of Faber that for any choice of nodes, there exists a continuous function not able to be approximated by the interpolating polynomial. The Chebyshev nodes are almost optimal in the sense that</p>
<div class="math notranslate nohighlight">
\[
\Lambda_{n, Chebyshev} &lt; \frac{2}{\pi}\log(n+1) + 1.
\]</div>
<p>The set of nodes that minimize <span class="math notranslate nohighlight">\(\Lambda_n\)</span> is difficult to compute. A slightly better set of nodes than Chebyshev nodes is the <strong>extended Chebyshev nodes</strong>:</p>
<div class="math notranslate nohighlight">
\[
\tilde{x}_j = \frac{\cos\left(\frac{2j+1}{2(n+1)\pi}\right)}{\cos\left(\frac{\pi}{2(n+1)}\right)}.
\]</div>
</section>
<section id="newton-form">
<h3>Newton Form<a class="headerlink" href="#newton-form" title="Link to this heading">#</a></h3>
<p>The Newton form is useful when we dynamically add interpolation nodes. Consider the following scenario: we already have found an interpolation polynomial <span class="math notranslate nohighlight">\(f_k\)</span> through <span class="math notranslate nohighlight">\((x_0, y_0)\)</span>, <span class="math notranslate nohighlight">\((x_1, y_1)\)</span>,<span class="math notranslate nohighlight">\(\dots, (x_k, y_k)\)</span>, then we are provided an addition pair <span class="math notranslate nohighlight">\((x_{k+1}, y_{k+1})\)</span>, how to effectively transform <span class="math notranslate nohighlight">\(f_k\)</span> to <span class="math notranslate nohighlight">\(f_{k+1}\)</span>? If we write</p>
<div class="math notranslate nohighlight">
\[
f_{k+1}(x) = f_k(x) + c_{k+1} (x - x_0)(x - x_1)\dots (x - x_{k}),
\]</div>
<p>then <span class="math notranslate nohighlight">\(f_{k+1}(x_j) = f_k(x_j)\)</span>, <span class="math notranslate nohighlight">\(j = 0, 1,\dots, k\)</span>, automatically. Therefore we only need to take care of <span class="math notranslate nohighlight">\(f_{k+1}(x_{k+1}) = y_{k+1}\)</span>, which means</p>
<div class="math notranslate nohighlight" id="equation-eq-ck">
<span class="eqno">(3)<a class="headerlink" href="#equation-eq-ck" title="Link to this equation">#</a></span>\[c_{k+1} = \frac{y_{k+1} - f_k(x_{k+1})}{\prod_{j=0}^k (x_{k+1} - x_j)}.\]</div>
<p>Such an inductive procedure produces the Newton form:</p>
<div class="math notranslate nohighlight" id="equation-eq-newton">
<span class="eqno">(4)<a class="headerlink" href="#equation-eq-newton" title="Link to this equation">#</a></span>\[f_n(x) = c_0 + c_1 ( x - x_0) + c_2 (x - x_0)(x - x_1)+\dots+c_{n}(x-x_0)\dots (x - x_{n-1}).\]</div>
<p>where the constant <span class="math notranslate nohighlight">\(c_j\)</span> depends on <span class="math notranslate nohighlight">\(x_0, x_1, \dots, x_{j}\)</span> only. The polynomials <span class="math notranslate nohighlight">\(\prod_{j=0}^k (x - x_j)\)</span> are called <strong>Newton polynomials</strong>. When the coefficients <span class="math notranslate nohighlight">\(c_k\)</span> are known, the Newton form <a class="reference internal" href="#equation-eq-newton">(4)</a> can be evaluated by the famous <strong>Horner’s scheme</strong>, which is</p>
<div class="math notranslate nohighlight" id="equation-eq-horner">
<span class="eqno">(5)<a class="headerlink" href="#equation-eq-horner" title="Link to this equation">#</a></span>\[    f_n(x) = c_0 + (x-x_0)(c_1 + (x-x_1)(c_2 + (x-x_2)(c_3 + \dots))),\]</div>
<p>the evaluation order starts from the innermost part <span class="math notranslate nohighlight">\(c_n (x -x_{n-1})\)</span>. This formulation has a complexity of <span class="math notranslate nohighlight">\(3n\)</span> flops.</p>
<div class="proof remark admonition" id="remark-15">
<p class="admonition-title"><span class="caption-number">Remark 5 </span></p>
<section class="remark-content" id="proof-content">
<p>The computation of <span class="math notranslate nohighlight">\(c_k\)</span> is not cheap from <a class="reference internal" href="#equation-eq-ck">(3)</a>. A naive algorithm with Horner’s scheme roughly takes <span class="math notranslate nohighlight">\(5n^2/2+\cO(n)\)</span> flops to compute all coefficients. The <strong>divided differences</strong> is a better way to compute <span class="math notranslate nohighlight">\(c_k\)</span>.</p>
</section>
</div><div class="proof definition admonition" id="definition-16">
<p class="admonition-title"><span class="caption-number">Definition 5 </span></p>
<section class="definition-content" id="proof-content">
<p>Let the interpolation nodes be <span class="math notranslate nohighlight">\(\{x_0, x_1, \dots, x_n\}\)</span>, the <strong>divided differences</strong> are defined recursively as follows (the square bracket is used to distinguish from the usual bracket):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
        f[x_j] &amp;:= f(x_j),\\
    f[x_{j}, \dots, x_{j+k}] &amp;:= \frac{f[x_{j+1},\dots, x_{j+k}] - f[x_j,\dots, x_{j+k-1}]}{x_{j+k} - x_{j}},
    \end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(0\le j, k\le n\)</span> and <span class="math notranslate nohighlight">\(j+k\le n\)</span>.</p>
</section>
</div><p>The following example graph is helpful to understand the relationships among the divided differences.</p>
<div class="math notranslate nohighlight" id="equation-eq-alg-newton">
<span class="eqno">(6)<a class="headerlink" href="#equation-eq-alg-newton" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
    f[x_0] &amp;               &amp;\\ 
            &amp;\searrow       &amp;\\ 
    f[x_1] &amp;\to f[x_0, x_1]&amp; \\
            &amp;\searrow        &amp;\searrow&amp;\\ 
    f[x_2] &amp; \to f[x_1, x_2]&amp;\to&amp; f[x_0, x_1, x_2]\\
            &amp;\searrow        &amp;\searrow&amp; &amp;\searrow&amp;\\ 
    f[x_3] &amp;\to f[x_2, x_3] &amp;\to&amp; f[x_1, x_2, x_3] &amp;\to &amp; f[x_0, x_1, x_2, x_3]\\
            &amp;\searrow        &amp;\searrow &amp; &amp;\searrow&amp;  &amp;\searrow &amp;\\ 
    f[x_4] &amp; \to f[x_3, x_4]&amp;\to&amp; f[x_2, x_3, x_4] &amp;\to &amp; f[x_1, x_2, x_3, x_4] &amp;\to&amp;  f[x_0, x_1, x_2, x_3, x_4] 
\end{aligned}\end{split}\]</div>
<p>It is clear that computing all of the divided differences requires <span class="math notranslate nohighlight">\(\frac{3n^2}{2} +\cO(n)\)</span> flops. The following theorem is the main statement for the Newton form.</p>
<div class="proof theorem admonition" id="theorem-17">
<p class="admonition-title"><span class="caption-number">Theorem 9 </span></p>
<section class="theorem-content" id="proof-content">
<p>The interpolation polynomial <span class="math notranslate nohighlight">\(f_n\)</span> in Newton form is given by</p>
<div class="math notranslate nohighlight" id="equation-eq-newton-form">
<span class="eqno">(7)<a class="headerlink" href="#equation-eq-newton-form" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
    f_n(x) =\; &amp;f[x_0] + f[x_0, x_1](x-x_0) + \dots +\\
             &amp; f[x_0, \dots, x_n](x - x_0)(x - x_1)\dots (x - x_{n-1}).
\end{aligned}\end{split}\]</div>
<p>In other words, <span class="math notranslate nohighlight">\(c_k = f[x_0, \dots, x_k]\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. We prove this by induction. Assume the statement is true for <span class="math notranslate nohighlight">\(n\)</span> and interpolation node and corresponding values <span class="math notranslate nohighlight">\((x_0, f[x_0]), (x_1, f[x_1]), \dots, (x_n, f[x_n])\)</span>. For a new node and value <span class="math notranslate nohighlight">\((x_{n+1}, f[x_{n+1}])\)</span>, it is known from <a class="reference internal" href="#equation-eq-ck">(3)</a> that <span class="math notranslate nohighlight">\(c_{n+1}\)</span> is the coefficient of leading power. Let <span class="math notranslate nohighlight">\(g_n\)</span> be the interpolation polynomial in Newton form through nodes <span class="math notranslate nohighlight">\((x_1, f[x_1]), \dots, (x_{n+1}, f[x_{n+1}])\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
\psi(x)  := g_n(x)(x - x_0) - f_n(x)(x - x_{n+1})
\]</div>
<p>satisfies that <span class="math notranslate nohighlight">\(\psi(x_j) = f[x_j](x_{n+1} - x_0)\)</span> for <span class="math notranslate nohighlight">\(0\le j\le {n+1}\)</span>. Therefore</p>
<div class="math notranslate nohighlight">
\[
f_{n+1}(x) = \frac{g_n(x)(x - x_0) - f_n(x)(x - x_{n+1})}{x_{n+1} - x_0}
\]</div>
<p>The leading power’s coefficient is then</p>
<div class="math notranslate nohighlight">
\[
    \frac{f[x_1, \dots, x_{n+1}] - f[x_0, \dots, x_n]}{x_{n+1} - x_0} = f[x_0, x_1,\dots, x_{n+1}].
\]</div>
</div>
<div class="proof remark admonition" id="remark-18">
<p class="admonition-title"><span class="caption-number">Remark 6 </span></p>
<section class="remark-content" id="proof-content">
<p>The divided difference <span class="math notranslate nohighlight">\(f[x_j, \dots, x_{j+k}]\)</span> is the coefficient of leading power of the interpolating polynomial through <span class="math notranslate nohighlight">\((x_j, f[x_j]), \dots, (x_{j+k}, f[x_{j+k}])\)</span>. It can be shown that</p>
<div class="math notranslate nohighlight">
\[
f[x_j, \dots, x_{j+k}] = \frac{1}{k!}f^{(k)}(\xi)
\]</div>
<p>for some <span class="math notranslate nohighlight">\(\xi\in [a, b]\)</span>. See exercise.</p>
</section>
</div><div class="proof remark admonition" id="remark-19">
<p class="admonition-title"><span class="caption-number">Remark 7 </span></p>
<section class="remark-content" id="proof-content">
<p>The error estimate can be derived as</p>
<div class="math notranslate nohighlight">
\[
    f(x) - f_n(x) = f[x_0, x_1, \dots, x_n, x] (x-x_0)\dots (x - x_n).
\]</div>
</section>
</div><div class="proof remark admonition" id="remark-20">
<p class="admonition-title"><span class="caption-number">Remark 8 </span></p>
<section class="remark-content" id="proof-content">
<p>The Newton form <a class="reference internal" href="#equation-eq-newton-form">(7)</a> actually does not require distinct nodes. The divided difference can be defined as a limit for repeated nodes:</p>
<div class="math notranslate nohighlight">
\[
    f[x_0, x_0] = \lim_{x_1 \to x_0} \frac{f[x_1] - f[x_0]}{x_1 - x_0} = f'(x_0).
\]</div>
<p>Moreover, using Taylor expansion, <span class="math notranslate nohighlight">\(f[\underbrace{x_0,\dots, x_0}_{(k+1)\,\text{times}}] = \frac{1}{k!}f^{(k)}(x_0)\)</span>. However, in such case the divided differences are not possible to be computed if the derivative values are not provided. We will discuss this scenario later in Hermite interpolation polynomial.</p>
</section>
</div><div class="proof remark admonition" id="remark-21">
<p class="admonition-title"><span class="caption-number">Remark 9 </span></p>
<section class="remark-content" id="proof-content">
<p>The algorithm to compute the divided difference can be made more efficient with a single column to store the diagonal elements. <span class="math notranslate nohighlight">\(\leadsto\)</span> is representing the number is not changing.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\color{red}{f[x_0]} &amp;    \leadsto  \color{green}{f[x_0]}          &amp; \leadsto&amp; \color{cyan}{f[x_0]} &amp;\leadsto&amp; \color{blue}{f[x_0]} &amp;\leadsto&amp; \color{black}{f[x_0]}\\ 
&amp;\searrow       &amp;\\ 
\color{red}{f[x_1]} &amp;\to \color{green}{f[x_0, x_1]}&amp;  \leadsto&amp; \color{cyan}{f[x_0, x_1]}  &amp;\leadsto&amp; \color{blue}{f[x_0, x_1]} &amp;\leadsto&amp; \color{black}{f[x_0, x_1]}\\
&amp;\searrow        &amp;\searrow&amp;\\ 
\color{red}{f[x_2]} &amp; \to \color{green}{f[x_1, x_2]}&amp;\to&amp; \color{cyan}{f[x_0, x_1, x_2]} &amp;\leadsto&amp; \color{blue}{f[x_0, x_1, x_2]}&amp;\leadsto&amp; \color{black}{f[x_0, x_1, x_2]}\\
&amp;\searrow        &amp;\searrow&amp; &amp;\searrow&amp;\\ 
\color{red}{f[x_3]} &amp;\to \color{green}{f[x_2, x_3] }&amp;\to&amp; \color{cyan}{f[x_1, x_2, x_3]} &amp;\to &amp; \color{blue}{f[x_0, x_1, x_2, x_3]}&amp;\leadsto&amp; \color{black}{f[x_0, x_1, x_2, x_3]}\\
&amp;\searrow        &amp;\searrow &amp; &amp;\searrow&amp;  &amp;\searrow &amp;\\ 
\color{red}{f[x_4]} &amp; \to \color{green}{f[x_3, x_4]}&amp;\to&amp; \color{cyan}{f[x_2, x_3, x_4]} &amp;\to &amp; \color{blue}{f[x_1, x_2, x_3, x_4]} &amp;\to&amp;  f[x_0, x_1, x_2, x_3, x_4] 
\end{aligned}
\end{split}\]</div>
</section>
</div></section>
<section id="hermite-polynomial-interpolation">
<h3>Hermite Polynomial Interpolation<a class="headerlink" href="#hermite-polynomial-interpolation" title="Link to this heading">#</a></h3>
<p>The Lagrange polynomial interpolation only requires the values of the data function <span class="math notranslate nohighlight">\(h\)</span> at each node. It can be generalized when the derivative values of <span class="math notranslate nohighlight">\(h\)</span> are also available.</p>
<p>Let the tuple <span class="math notranslate nohighlight">\((h(x_j), h^{(1)}(x_j), \dots, h^{(m_j)}(x_j))\)</span> be the provided derivative values at the interpolation node <span class="math notranslate nohighlight">\(x_j\)</span>, <span class="math notranslate nohighlight">\(j=0,\dots, n\)</span> and <span class="math notranslate nohighlight">\(m_j\ge 0\)</span>. <span class="math notranslate nohighlight">\(N = \sum_{j=0}^n (m_j + 1)\)</span> is the total number of constraints. It can be shown that there exists a unique polynomial <span class="math notranslate nohighlight">\(H_{N-1}\in \Pi_{N-1}\)</span> satisfies</p>
<div class="math notranslate nohighlight">
\[
H_{N-1}^{(k)}(x_j) = y_j^k:= h^{(k)}(x_j),\quad j=0,\dots, n,\quad 0\le k\le m_j.
\]</div>
<p>This polynomial is called <strong>Hermite interpolation polynomial</strong>. The idea to construct the Hermite interpolation polynomial borrows from the Lagrange polynomials, which is to find basis <span class="math notranslate nohighlight">\(L_{jk}\)</span> such that</p>
<div class="math notranslate nohighlight" id="equation-eq-hermite-construction">
<span class="eqno">(8)<a class="headerlink" href="#equation-eq-hermite-construction" title="Link to this equation">#</a></span>\[\begin{split}    \frac{d^p}{d x^p}L_{jk}(x_l) = \begin{cases}
        1, &amp; l = j, k = p\\
        0, &amp; \text{otherwise}.
    \end{cases}\end{split}\]</div>
<p>Once these polynomials are obtained, the Hermite interpolation is straightforward:</p>
<div class="math notranslate nohighlight">
\[H_{N-1}(x) = \sum_{j=0}^n \sum_{k=0}^{m_j} y_j^k L_{jk}(x).\]</div>
<p>Its uniqueness can be concluded from the linearly independence of the basis <span class="math notranslate nohighlight">\(L_{jk}\)</span>. However, the construction method in <a class="reference internal" href="#equation-eq-hermite-construction">(8)</a> is not the simplest. It is known that the Newton form <a class="reference internal" href="#equation-eq-newton-form">(7)</a> works for repeated nodes as long as the diagram’s diagonal <a class="reference internal" href="#equation-eq-alg-newton">(6)</a> can be filled. Therefore, we can arrange the nodes</p>
<div class="math notranslate nohighlight">
\[
\underbrace{x_0,\dots, x_0}_{(m_0+1)\,\text{times}}, \quad \underbrace{x_1,\dots, x_1}_{(m_1+1)\,\text{times}}, \quad \dots,\quad  \underbrace{x_n,\dots, x_n}_{(m_n+1)\,\text{times}}
\]</div>
<p>In this way, all of the necessary divided differences can be computed. See exercise.</p>
<div class="proof remark admonition" id="remark-22">
<p class="admonition-title"><span class="caption-number">Remark 10 </span></p>
<section class="remark-content" id="proof-content">
<p>The error estimate for Hermite polynomial interpolation will be the same as the Newton form case.</p>
</section>
</div></section>
</section>
<section id="trigonometric-interpolation">
<h2>Trigonometric Interpolation<a class="headerlink" href="#trigonometric-interpolation" title="Link to this heading">#</a></h2>
<p>Periodic functions occur in many applications, that is, <span class="math notranslate nohighlight">\(f(x + T) = f(x)\)</span>, <span class="math notranslate nohighlight">\(x\in \mathbb{R}\)</span> for some <span class="math notranslate nohighlight">\(T &gt; 0\)</span>. For example, a closed planar curve can be parameterized as a periodic function naturally. The polynomial interpolation does not suit periodic functions, this is because polynomials will eventually go to infinity as <span class="math notranslate nohighlight">\(x\to\infty\)</span>. The most used interpolation for the periodic function is the \emph{trigonometric polynomial interpolation}. In the following, we assume the period <span class="math notranslate nohighlight">\(T = 2\pi\)</span> without loss of generality.</p>
<section id="fourier-series">
<h3>Fourier Series<a class="headerlink" href="#fourier-series" title="Link to this heading">#</a></h3>
<div class="proof definition admonition" id="definition-23">
<p class="admonition-title"><span class="caption-number">Definition 6 </span></p>
<section class="definition-content" id="proof-content">
<p>For <span class="math notranslate nohighlight">\(n\ge 0\)</span>, we defined <span class="math notranslate nohighlight">\(F_n\)</span> the space of trigonometric polynomials</p>
<div class="math notranslate nohighlight">
\[
    F_n := \{f(x) \mid f(x) = \frac{a_0}{2} + \sum_{k=1}^n a_k \cos kx + \sum_{k=1}^n b_k \sin kx,\; a_k, b_k\in\mathbb{R}\}.
\]</div>
<p>The coefficients <span class="math notranslate nohighlight">\(a_0,\dots, a_n\)</span>, <span class="math notranslate nohighlight">\(b_1,\dots, b_n\)</span> can be also chosen as complex numbers. <span class="math notranslate nohighlight">\(f\in F_n\)</span> is said to be of degree <span class="math notranslate nohighlight">\(n\)</span> if <span class="math notranslate nohighlight">\(|a_n| + |b_n| &gt; 0\)</span>.</p>
</section>
</div><p>The concept of <strong>degree</strong> here can be validated by the addition theorem of trigonometric functions. For instance, if <span class="math notranslate nohighlight">\(f_1\in F_k\)</span>, <span class="math notranslate nohighlight">\(f_2\in F_l\)</span>, then <span class="math notranslate nohighlight">\(f_1 f_2 \in F_{k+l}\)</span>. In the next, we discuss the uniqueness of the interpolation with the trigonometric polynomial.</p>
<div class="proof lemma admonition" id="lemma-24">
<p class="admonition-title"><span class="caption-number">Lemma 4 </span></p>
<section class="lemma-content" id="proof-content">
<p>A trigonometric polynomial <span class="math notranslate nohighlight">\(f\in F_n\)</span> that has more than <span class="math notranslate nohighlight">\(2n\)</span> zeros in <span class="math notranslate nohighlight">\([0, 2\pi)\)</span> must vanish identically.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Rewrite the trigonometric function in the form of</p>
<div class="math notranslate nohighlight" id="equation-eq-complex">
<span class="eqno">(9)<a class="headerlink" href="#equation-eq-complex" title="Link to this equation">#</a></span>\[    f_n(x) = \sum_{k=-n}^{n} \gamma_k e^{ik x}. \]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma_0 = \frac{1}{2}a_0\)</span> and <span class="math notranslate nohighlight">\(\gamma_{k} = \frac{1}{2}(a_k - ib_k)\)</span> and <span class="math notranslate nohighlight">\(\gamma_{-k} = \frac{1}{2}(a_k + i b_k)\)</span>, <span class="math notranslate nohighlight">\(k=1,\dots, n\)</span>. Then substitute <span class="math notranslate nohighlight">\(z = e^{ix}\)</span> and set</p>
<div class="math notranslate nohighlight">
\[
    p(z) = \sum_{k = -n}^n \gamma_k z^{n + k}, 
\]</div>
<p>one can rewrite <span class="math notranslate nohighlight">\(f_n(x) = z^{-n} p(z)\)</span>. If <span class="math notranslate nohighlight">\(f_n(x)\)</span> has more than <span class="math notranslate nohighlight">\(2n\)</span> zeros, then <span class="math notranslate nohighlight">\(p(z)\)</span> has more than <span class="math notranslate nohighlight">\(2n\)</span> zeros, which is a contradiction since <span class="math notranslate nohighlight">\(p(z)\)</span> is a polynomial of degree <span class="math notranslate nohighlight">\(2n\)</span>.</p>
</div>
<div class="proof remark admonition" id="remark-25">
<p class="admonition-title"><span class="caption-number">Remark 11 </span></p>
<section class="remark-content" id="proof-content">
<p>Since <span class="math notranslate nohighlight">\(\sin nx\in F_n\)</span> has <span class="math notranslate nohighlight">\(2n\)</span> zeros <span class="math notranslate nohighlight">\(\frac{\pi j}{n}\)</span>, <span class="math notranslate nohighlight">\(j=0,\dots, 2n-1\)</span>, it means to uniquely determine a trigonometric polynomial in <span class="math notranslate nohighlight">\(F_n\)</span>, exactly <span class="math notranslate nohighlight">\(2n+1\)</span> values are needed. This is also known as the Nyquist sampling theorem.</p>
</section>
</div><p>A direct corollary is the linear independence of the functions <span class="math notranslate nohighlight">\(1\)</span>, <span class="math notranslate nohighlight">\(\cos k x\)</span> and <span class="math notranslate nohighlight">\(\sin k x\)</span>, <span class="math notranslate nohighlight">\(k = 1, \dots n\)</span>, these <span class="math notranslate nohighlight">\((2n+1)\)</span> functions form a natural basis for the trigonometric polynomial space <span class="math notranslate nohighlight">\(F_n\)</span>.</p>
<div class="proof corollary admonition" id="corollary-26">
<p class="admonition-title"><span class="caption-number">Corollary 4 </span></p>
<section class="corollary-content" id="proof-content">
<p>The functions <span class="math notranslate nohighlight">\(1, \cos kx, \sin kx\)</span>, <span class="math notranslate nohighlight">\(k=1,\dots, n\)</span> are linearly independent on <span class="math notranslate nohighlight">\(C([0, 2\pi])\)</span>, hence <span class="math notranslate nohighlight">\(F_n\)</span> is a <span class="math notranslate nohighlight">\((2n+1)\)</span> dimensional space.</p>
</section>
</div><p>To determine the coefficients <span class="math notranslate nohighlight">\(a_k, b_k\)</span> from <span class="math notranslate nohighlight">\((2n+1)\)</span> data paris <span class="math notranslate nohighlight">\((x_j, y_j)\)</span>, <span class="math notranslate nohighlight">\(j=0, \dots, 2n\)</span>. We simply follow the idea of Lagrange polynomials by creating the basis polynomial <span class="math notranslate nohighlight">\(l_k(x)\)</span> such that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
l_k(x_j) = \begin{cases}
    1, &amp;j = k,\\
    0,&amp;\text{otherwise}.
\end{cases}
\end{split}\]</div>
<div class="proof remark admonition" id="remark-27">
<p class="admonition-title"><span class="caption-number">Remark 12 </span></p>
<section class="remark-content" id="proof-content">
<p>A natural idea is replace <span class="math notranslate nohighlight">\(x - x_j\)</span> in the Lagrange basis by <span class="math notranslate nohighlight">\(\sin(x - x_j)\)</span> and produce something like</p>
<div class="math notranslate nohighlight">
\[
\prod_{j=0, j\neq k}^{2n}\frac{\sin(x - x_j)}{\sin(x_k - x_j)},
\]</div>
<p>but <span class="math notranslate nohighlight">\(\sin(x - x_j)\)</span> has two roots on <span class="math notranslate nohighlight">\([0, 2\pi)\)</span>, therefore we need to rescale it to <span class="math notranslate nohighlight">\([0, \pi)\)</span>.</p>
</section>
</div><div class="proof theorem admonition" id="THM-TRIG">
<p class="admonition-title"><span class="caption-number">Theorem 10 </span></p>
<section class="theorem-content" id="proof-content">
<p>Let the basis trigonometric polynomial</p>
<div class="math notranslate nohighlight">
\[
l_k(x) =\prod_{j=0, j\neq k}^{2n}\frac{\sin(\frac{x - x_j}{2})}{\sin(\frac{x_k - x_j}{2})} ,
\]</div>
<p>then the interpolation trigonometric polynomial is</p>
<div class="math notranslate nohighlight">
\[
f_n(x) = \sum_{k=0}^{2n} y_k l_k(x).
\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. It remains to show <span class="math notranslate nohighlight">\(l_k\in F_n\)</span>. This can be seen by splitting <span class="math notranslate nohighlight">\(l_k\)</span> into <span class="math notranslate nohighlight">\(n\)</span> pairs, each pair takes the form of</p>
<div class="math notranslate nohighlight">
\[
    \sin(\frac{x-x_0}{2})\sin(\frac{x-x_1}{2}) = \frac{1}{2}\cos\left( \frac{x_0 - x_1}{2}\right) - \frac{1}{2}\cos\left(\frac{2x - x_0 - x_1}{2}\right)\in F_1.
\]</div>
</div>
<p>Computationally, we can reuse the previously known barycentric form but there exist better methods. For simplicity, we consider the equal space nodes in the following (non-uniform nodes could achieve the same complexity though).</p>
<div class="math notranslate nohighlight">
\[
    x_j = \frac{2\pi j}{2n + 1}, \quad j = 0, \dots, 2n.
\]</div>
<p>We will try to locate the coefficients <span class="math notranslate nohighlight">\(\gamma_k\)</span> in the complex form (see <a class="reference internal" href="#equation-eq-complex">(9)</a>) from the interpolation conditions.</p>
<div class="math notranslate nohighlight">
\[
    f_n(x_j) = y_j = \sum_{k=-n}^n \gamma_k e^{i k x_j}.
\]</div>
<p>Use the property of the functions <span class="math notranslate nohighlight">\(e^{ikx_j}\)</span> that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \sum_{k=0}^{2n} e^{ik x_j} = \begin{cases}
        2n+1, &amp; k = 0\\
        0, &amp;\text{otherwise}.
    \end{cases}
\end{split}\]</div>
<p>It is not difficult to derive</p>
<div class="math notranslate nohighlight">
\[
    \sum_{j = 0}^{2n} y_j e^{-im x_j} = \sum_{k=-n}^{n} \gamma_k \sum_{j=0}^{2n} e^{i (k - m) x_j} = \gamma_{m} (2n+1).  
\]</div>
<p>Therefore, we can compute</p>
<div class="math notranslate nohighlight" id="equation-eq-gamma">
<span class="eqno">(10)<a class="headerlink" href="#equation-eq-gamma" title="Link to this equation">#</a></span>\[    \gamma_m = \frac{1}{2n+1}  \sum_{j = 0}^{2n} y_j e^{-im x_j}.\]</div>
<p>When the coefficients <span class="math notranslate nohighlight">\(\gamma_m\)</span> are known, a Horner’s scheme can be employed to evaluate the trigonometric polynomial in <span class="math notranslate nohighlight">\(\cO(n)\)</span> time complexity.  However, naive computing of all of the coefficients <span class="math notranslate nohighlight">\(\gamma_k\)</span> will cost <span class="math notranslate nohighlight">\(\cO(n^2)\)</span> flops. The fast Fourier transform can reduce the time complexity to <span class="math notranslate nohighlight">\(\cO(n\log n)\)</span>.</p>
</section>
<section id="fast-fourier-transform">
<h3>Fast Fourier Transform<a class="headerlink" href="#fast-fourier-transform" title="Link to this heading">#</a></h3>
<p>The discrete Fourier transform <span class="math notranslate nohighlight">\(\texttt{DFT}\)</span> of a vector <span class="math notranslate nohighlight">\(\mathbf{a} = (a_0, \dots, a_{n-1})\)</span> is to evaluate the following vector:</p>
<div class="math notranslate nohighlight">
\[
\texttt{DFT}(\mathbf{a})_{k} := \frac{1}{n}\sum_{j=0}^{n-1} a_je^{-2\pi i jk/n},\quad k = 0,\dots, n-1.
\]</div>
<p>This is the exact formula to compute the coefficients for the trigonometric interpolation polynomial. Such transform is most efficiently calculated through the fast Fourier transform (<span class="math notranslate nohighlight">\(\texttt{fft}\)</span>).
The fast Fourier transform exploits the symmetry in <span class="math notranslate nohighlight">\(e^{2\pi i j/n}\)</span> when <span class="math notranslate nohighlight">\(n\)</span> is the power of two using divide-and-conquer. Let <span class="math notranslate nohighlight">\(\omega = e^{-2\pi i/n}\)</span> and <span class="math notranslate nohighlight">\(c_k\)</span> be</p>
<div class="math notranslate nohighlight">
\[
    c_k = \frac{1}{n}\sum_{j=0}^{n-1} y_j\omega^{jk},\quad k = 0,\dots, n-1.
\]</div>
<p>Let <span class="math notranslate nohighlight">\(m = n/2\in \mathbb{N}\)</span>, then <span class="math notranslate nohighlight">\(\omega^n = 1\)</span>, <span class="math notranslate nohighlight">\(\omega^m = -1\)</span>. We can separate <span class="math notranslate nohighlight">\(c_k\)</span> into two parts with even <span class="math notranslate nohighlight">\(j\)</span> and odd <span class="math notranslate nohighlight">\(j\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-ck2">
<span class="eqno">(11)<a class="headerlink" href="#equation-eq-ck2" title="Link to this equation">#</a></span>\[    c_k = \frac{1}{2} A_k + \frac{1}{2} B_k \omega^k,\quad c_{k+m} = \frac{1}{2} A_k - \frac{1}{2} B_k \omega^k \]</div>
<p>where</p>
<div class="math notranslate nohighlight" id="equation-eq-ak-bk">
<span class="eqno">(12)<a class="headerlink" href="#equation-eq-ak-bk" title="Link to this equation">#</a></span>\[\begin{aligned}
    A_k = \frac{1}{m} \sum_{j=0}^{m-1} y_{2j} (\omega^2)^{jk},\quad B_k = \frac{1}{m} \sum_{j=0}^{m-1} y_{2j+1} (\omega^2)^{jk},
\end{aligned}\]</div>
<p>both <span class="math notranslate nohighlight">\(A_k\)</span> and <span class="math notranslate nohighlight">\(B_k\)</span> are in the same form and similar to <span class="math notranslate nohighlight">\(c_k\)</span>, but with only half of the terms in summation. This implies a recursive algorithm. Suppose <span class="math notranslate nohighlight">\(A_k\)</span> and <span class="math notranslate nohighlight">\(B_k\)</span>, <span class="math notranslate nohighlight">\(0\le k\le m-1\)</span> can be computed with <span class="math notranslate nohighlight">\(f(m)\)</span> operations each, then</p>
<div class="math notranslate nohighlight">
\[
   f(n) =  f(2m) = 2 f(m) + 4m
\]</div>
<p>The second term includes <span class="math notranslate nohighlight">\(2m\)</span> multiplications and <span class="math notranslate nohighlight">\(2m\)</span> additions in <a class="reference internal" href="#equation-eq-ck2">(11)</a>. Therefore</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
        f(n)&amp; = 2f(\frac{n}{2}) + 2n \\
&amp;= 4f(\frac{n}{4}) + 2 n + 2n \\
&amp;=\dots \\
&amp;= n f(1) + \underbrace{2n + \dots + 2n}_{\log_2 n \text{ times}} = 2n \log_2 n.
    \end{aligned}
\end{split}\]</div>
<p>since <span class="math notranslate nohighlight">\(f(1) = 0\)</span>, no computation is needed in this case.  The <span class="math notranslate nohighlight">\(\texttt{fft}\)</span> is usually a standard routine in modern scientific computing software.</p>
</section>
<section id="interpolation-error-of-trigonometric-polynomial">
<h3>Interpolation Error of Trigonometric Polynomial<a class="headerlink" href="#interpolation-error-of-trigonometric-polynomial" title="Link to this heading">#</a></h3>
<p>The <span class="math notranslate nohighlight">\(L^2\)</span> error estimate will be discussed at a later point. In this part, we only focus on the <span class="math notranslate nohighlight">\(L^{\infty}\)</span> error estimate.</p>
<div class="proof theorem admonition" id="theorem-29">
<p class="admonition-title"><span class="caption-number">Theorem 11 </span></p>
<section class="theorem-content" id="proof-content">
<p>If <span class="math notranslate nohighlight">\(f\in C^2(\mathbb{R})\)</span> is a <span class="math notranslate nohighlight">\(2\pi\)</span>-period function, then the trigonometric interpolation polynomial with <span class="math notranslate nohighlight">\(2n+1\)</span> equally spaced nodes converges uniformly as the number of nodes tends to infinity.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Since <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable, its Fourier series converges to <span class="math notranslate nohighlight">\(f\)</span> uniformly. Let <span class="math notranslate nohighlight">\(f(x), g(x)\)</span> be</p>
<div class="math notranslate nohighlight">
\[
f(x) = \sum_{s = -\infty}^{\infty} \gamma_s e^{is x},\quad g(x) = \sum_{s = -n}^n \gamma_s e^{is x},
\]</div>
<p>respectively and denote <span class="math notranslate nohighlight">\(h(x) = f(x)- g(x) = \sum_{|s| &gt; n} \gamma_s e^{isx}\)</span> the reminder. Use integration by parts twice,</p>
<div class="math notranslate nohighlight">
\[
\gamma_s = \frac{1}{2\pi}\int_{0}^{2\pi} f(x) e^{-isx} dx = -\frac{1}{2\pi}\frac{1}{s^2}\int_{0}^{2\pi} f''(x) e^{-isx} dx \le \frac{1}{2\pi s^2}\|f''\|_{\infty}.
\]</div>
<p>On the other hand, the interpolation polynomial</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    f_n(x)&amp; = \sum_{m=-n}^n \left(\frac{1}{2n+1}\sum_{j=0}^{2n} y_j e^{-im x_j}\right) e^{im x}  \\
        &amp;=\sum_{m=-n}^n \left(\frac{1}{2n+1}\sum_{j=0}^{2n} ( g(x_j) + h(x_j) ) e^{-im x_j}\right) e^{im x} \\
        &amp;= \sum_{m=-n}^n \left(\frac{1}{2n+1}\sum_{j=0}^{2n} ( \sum_{s = -n}^n \gamma_s  e^{is x_j }+ h(x_j) ) e^{-im x_j}\right) e^{im x} \\
        &amp;=g(x)+ \sum_{m=-n}^n \left(\frac{1}{2n+1}\sum_{j=0}^{2n} h(x_j) e^{-im x_j}\right)e^{im x} \\
        &amp;=g(x)+ \frac{1}{2n+1} \sum_{j=0}^{2n} h(x_j) \frac{\sin((2n+1)(x-x_j)/2)}{\sin((x-x_j)/2)}
\end{aligned}
\end{split}\]</div>
<p>Therefore</p>
<div class="math notranslate nohighlight" id="equation-eq-diff">
<span class="eqno">(13)<a class="headerlink" href="#equation-eq-diff" title="Link to this equation">#</a></span>\[        |f - f_n| \le \|h\|_{\infty} + \|h\|_{\infty}  \frac{1}{2n+1} \sum_{j=0}^{2n} \left|\frac{\sin((2n+1)(x-x_j)/2)}{\sin((x-x_j)/2)}\right|.\]</div>
<p>It is simple to derive <span class="math notranslate nohighlight">\(\|h\|_{\infty} \le \sum_{|s|&gt;n}|\gamma_s| \le \frac{1}{n\pi}\|f''\|_{\infty}\)</span>. We only need to estimate</p>
<div class="math notranslate nohighlight">
\[ 
\sum_{j=0}^{2n} \left|\frac{\sin((2n+1)(x-x_j)/2)}{\sin((x-x_j)/2)}\right|
\]</div>
<p>Separate the nodes into two groups: The first group with <span class="math notranslate nohighlight">\(|x - x_j| &lt; \frac{2\pi}{2n+1}\)</span>, the absolute value is bounded by <span class="math notranslate nohighlight">\((2n+1)\)</span>, there are at most <span class="math notranslate nohighlight">\(3\)</span> nodes lying in this region, thus the contribution is at most <span class="math notranslate nohighlight">\(\cO(n)\)</span>. The second group is <span class="math notranslate nohighlight">\(\pi\ge |x - x_j| \ge \frac{2\pi}{2n+1}\)</span>, while the rest is symmetric, then one can estimate</p>
<div class="math notranslate nohighlight">
\[
\left|\frac{\sin((2n+1)(x-x_j)/2)}{\sin((x-x_j)/2)}\right| \le \frac{\pi}{|x - x_j|}
\]</div>
<p>where the inequality <span class="math notranslate nohighlight">\(\sin x \ge \frac{2}{\pi}x\)</span> for <span class="math notranslate nohighlight">\(0\le x\le \frac{\pi}{2}\)</span>, then this part will be at most <span class="math notranslate nohighlight">\(\cO(n\log n)\)</span>, the total contribution is bounded by <span class="math notranslate nohighlight">\(\cO(n\log n)\)</span>. Then <a class="reference internal" href="#equation-eq-diff">(13)</a> can be bounded by</p>
<div class="math notranslate nohighlight">
\[
    |f - f_n|\le \|f''\|_{\infty} \cO\left(\frac{\log n}{n}\right)\to 0,\quad \text{as } n\to \infty.
\]</div>
</div>
<p>The above result can be extended to the case of Hölder continuous function, see the work of Dunham Jackson (1913).</p>
</section>
</section>
<section id="spline-interpolation">
<h2>Spline Interpolation<a class="headerlink" href="#spline-interpolation" title="Link to this heading">#</a></h2>
<p>It has been seen that increasing the number of interpolation nodes will not always help to improve the approximation. The spline interpolation is to conquer this issue by using the piecewise low-degree polynomials.</p>
<div class="proof definition admonition" id="definition-30">
<p class="admonition-title"><span class="caption-number">Definition 7 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(x_0, \dots, x_n\)</span> be the distinct nodes on <span class="math notranslate nohighlight">\([a, b]\)</span> such that <span class="math notranslate nohighlight">\(a = x_0 &lt;\dots &lt; x_n = b\)</span>. The piecewise defined function <span class="math notranslate nohighlight">\(s_k(x)\)</span> on the interval <span class="math notranslate nohighlight">\([a, b]\)</span> is a spline of degree <span class="math notranslate nohighlight">\(k\)</span> to the nodes if</p>
<div class="math notranslate nohighlight">
\[
    s_k|_{[x_j, x_{j+1}]} \in \Pi_k, \quad s_k\in C^{k-1}([a, b]).
\]</div>
<p>The spline function <span class="math notranslate nohighlight">\(s_k\)</span> is <span class="math notranslate nohighlight">\((k-1)\)</span>-times continuously differentiable and piecewise polynomial of degree <span class="math notranslate nohighlight">\(k\)</span>.</p>
</section>
</div><p>Then the space of splines <span class="math notranslate nohighlight">\(s_k\)</span> will be <span class="math notranslate nohighlight">\((n + k)\)</span> dimension: each interval has <span class="math notranslate nohighlight">\((k+1)\)</span> dimensions, each interface imposes <span class="math notranslate nohighlight">\(k\)</span> constraints, therefore <span class="math notranslate nohighlight">\(n (k+ 1) - (n-1) k = n + k\)</span> dimensions. This shows that to determine a spline on the nodes uniquely, we will require <span class="math notranslate nohighlight">\(n+1\)</span> interpolation values and <span class="math notranslate nohighlight">\(k-1\)</span> additional constraints. Usual choices are</p>
<ul class="simple">
<li><p>periodic splines. <span class="math notranslate nohighlight">\(s_k^{(m)}(a) = s_k^{(m)}(b)\)</span> for <span class="math notranslate nohighlight">\(m = 0, 1, \dots, k-1\)</span>.</p></li>
<li><p>natural splines. <span class="math notranslate nohighlight">\(s_k^{(l+j)}(a) = s_k^{(l+j)}(b) = 0\)</span>, <span class="math notranslate nohighlight">\(j = 0, 1,\dots, l-2\)</span> and <span class="math notranslate nohighlight">\(k = 2l-1\)</span> with <span class="math notranslate nohighlight">\(l\ge 2\)</span>.</p></li>
</ul>
<p>In the following, we discuss some useful examples of spline.</p>
<section id="linear-spline">
<h3>Linear Spline<a class="headerlink" href="#linear-spline" title="Link to this heading">#</a></h3>
<p>The linear splines are a special case of splines. It uses piecewise linear polynomials on each subinterval and does not impose any derivative continuity. Let <span class="math notranslate nohighlight">\(y_j\)</span> be the interpolation values at nodes <span class="math notranslate nohighlight">\(x_j\)</span>, respectively. The interpolation has an explicit form:</p>
<div class="math notranslate nohighlight">
\[
    s_1(x) = y_{j-1} + \frac{x - x_{j-1}}{x_j - x_{j-1}} y_j
\]</div>
<p>on the interval <span class="math notranslate nohighlight">\([x_{j-1}, x_j]\)</span>. It can be represented as a linear combination of the ``hat’’ basis function <span class="math notranslate nohighlight">\(\theta_j(x)\)</span>, defined by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \theta_j(x) = \begin{cases}
        \frac{x - x_{{j-1}}}{x_j - x_{j-1}}, &amp; x\in [x_{j-1}, x_{j}],\quad 1\le j\le n\\
         \frac{x - x_{j+1}}{x_j - x_{j+1}}, &amp; x\in [x_j, x_{j+1}],\quad 0\le j\le n-1\\
         0, &amp; \text{otherwise}
    \end{cases}
\end{split}\]</div>
<p>then <span class="math notranslate nohighlight">\(s_1(x)\)</span> can be written as</p>
<div class="math notranslate nohighlight">
\[s_1(x) = \sum_{j=0}^n \theta_j(x) y_j.\]</div>
<p>The interpolation error can be derived directly from the previous theory for two interpolation nodes. Let <span class="math notranslate nohighlight">\(f\in C^2([a, b])\)</span>, then on <span class="math notranslate nohighlight">\([x_{j-1}, x_j]\)</span>, the interpolation error is</p>
<div class="math notranslate nohighlight">
\[
    \frac{1}{2!}f''(\xi) (x - x_{j-1})(x - x_j) \le \frac{1}{8} \|f''\|_{\infty} |x_j - x_{j-1}|^2.
\]</div>
<p>Therefore, the interpolation error on <span class="math notranslate nohighlight">\([a, b]\)</span> is <span class="math notranslate nohighlight">\(\frac{1}{8} \|f''\|_{\infty} h^2\)</span>, where <span class="math notranslate nohighlight">\(h = \max|x_j - x_{j-1}|\)</span>. Once <span class="math notranslate nohighlight">\(f''\)</span> is not uniformly bounded or even <span class="math notranslate nohighlight">\(f'\)</span> is not well-defined somewhere, e.g., <span class="math notranslate nohighlight">\(f\in C^{0,\alpha}[a, b]\)</span>, the interpolation error will be replaced by the modulus of continuity. On <span class="math notranslate nohighlight">\(x\in [x_j, x_{j+1}]\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    |s_1(x) - f(x)| &amp;= \left| \frac{ (x_{j+1} - x) y_j+ (x - x_j) y_{j+1}}{x_{j+1} - x_j} - f(x) \right| \\
    &amp;=  \left| \frac{ (x_{j+1} - x) (y_j - f(x)) + (x - x_j) (y_{j+1}-f(x))}{x_{j+1} - x_j}  \right| \\ &amp;\le \omega(f; |x_{j+1} - x_j|),
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\omega(f;\tau)\)</span> is the modulus of continuity of <span class="math notranslate nohighlight">\(f\)</span>.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="root_finding.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Root Finding</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#polynomial-interpolation">Polynomial Interpolation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lagrange-polynomial">Lagrange Polynomial</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpolation-error">Interpolation Error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#runge-s-phenomenon">Runge’s Phenomenon</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpolation-remainder-theory">Interpolation Remainder Theory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chebyshev-interpolation">Chebyshev Interpolation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stability-of-polynomial-interpolation">Stability of Polynomial Interpolation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-form">Newton Form</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hermite-polynomial-interpolation">Hermite Polynomial Interpolation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#trigonometric-interpolation">Trigonometric Interpolation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fourier-series">Fourier Series</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fast-fourier-transform">Fast Fourier Transform</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpolation-error-of-trigonometric-polynomial">Interpolation Error of Trigonometric Polynomial</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spline-interpolation">Spline Interpolation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-spline">Linear Spline</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yimin Zhong
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>